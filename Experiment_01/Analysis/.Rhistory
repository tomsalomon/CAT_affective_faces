PHQ_data$SubjectID_num=as.numeric(substr(PHQ_data$SubjectID_name,nchar(PHQ_data$SubjectID_name)-2,nchar(PHQ_data$SubjectID_name)))
PHQ_data$SubjectID=PHQ_data$SubjectID_num
Data=merge(Data,PHQ_data[c("SubjectID","PHQ")],by = "SubjectID")
model_data=melt(tapply(Data$Rank, list(SubjectID=Data$SubjectID,PHQ=Data$PHQ,Face_Affect=Data$Face_Affect), mean,na.rm=T),id="Subject")
model_data=model_data[!is.na(model_data$value),]
model_data=model_data[model_data$Face_Affect=="Happy",]
model_data$PHQ_ordinal=rank(model_data$PHQ)
model_data$value_ordinal=rank(model_data$value)
model_data$SubjectID_num=model_data$SubjectID
model_data=merge(model_data,data_by_subject_group[,c("SubjectID_num","Happy_Faces","Neutral_Faces","Happy_Faces_rank","Neutral_Faces_rank")],by="SubjectID_num")
names_tmp=colnames(model_data)
names_tmp[names_tmp=="value"]="Happy_Faces_initial_prefernces"
names_tmp[names_tmp=="value_ordinal"]="Happy_Faces_initial_prefernces_ordinal"
names_tmp[names_tmp=="Happy_Faces"]="Happy_Faces_CAT_effect"
names_tmp[names_tmp=="Happy_Faces_rank"]="Happy_Faces_CAT_effect_ordinal"
colnames(model_data)=names_tmp
model_PHQ_by_CAT_and_prefernces=lm(PHQ_ordinal ~ Happy_Faces_CAT_effect_ordinal + Happy_Faces_initial_prefernces_ordinal,data=model_data)
model_PHQ_by_CAT=lm(PHQ_ordinal ~ Happy_Faces_CAT_effect_ordinal ,data=model_data)
model_PHQ_by_prefernces=lm(PHQ_ordinal ~ Happy_Faces_initial_prefernces_ordinal,data=model_data)
summary(lm.beta(model_PHQ_by_CAT_and_prefernces))
anova(model_PHQ_by_CAT_and_prefernces,model_PHQ_by_prefernces)
# 3D plot
colors=model_data$PHQ_ordinal
colors=rank(model_data$Happy_Faces_initial_prefernces_ordinal*model_data$Happy_Faces_CAT_effect_ordinal)
colfunc<-colorRampPalette(c("red","yellow"))
colormap=colfunc(length(unique(colors)))
colors=colormap[c(colors)]
dev.new()
s3d <- scatterplot3d(model_data$Happy_Faces_initial_prefernces_ordinal, model_data$Happy_Faces_CAT_effect_ordinal, model_data$PHQ_ordinal, type = "h", lab = c(3, 3),
pch=19,color=colors,
xlab="Initial Preference Bias towards Happy Faces (Rank)",
ylab="CAT Effect - Happy Faces (Rank)",
zlab="PHQ (Rank)") # lab: number of tickmarks on x-/y-axes
s3d$plane3d(model_PHQ_by_CAT_and_prefernces)
# # 3D animation
#  for (i in seq(1,360,2)) {
#
#    #saves the plot as a .png file in the working directory
#  png(paste(1000+i,".png",sep=""))
#    s3d <- scatterplot3d(model_data$Happy_Faces_initial_prefernces_ordinal, model_data$Happy_Faces_CAT_effect_ordinal, model_data$PHQ_ordinal, type = "h", lab = c(3, 3),
#                         pch=19,color=colors,
#                         xlab="Initial Preference Bias towards Happy Faces (Rank)",
#                         ylab="CAT Effect - Happy Faces (Rank)",
#                         zlab="PHQ (Rank)",
#                         angle=i) # lab: number of tickmarks on x-/y-axes
#    s3d$plane3d(model_PHQ_by_CAT_and_prefernces)
#   dev.off()
#  }
# my_command <- 'convert *.png -delay 1 -loop 0 3d.gif'
# system(my_command)
# ANIMATE: https://davetang.org/muse/2015/02/12/animated-plots-using-r/
#svm_dat=data_by_subject_group[ , c("Happy_Faces","Neutral_Faces","PHQ_binary")]
#svm_fit=svm(PHQ_binary ~ .,data=svm_dat,kernel="linear",cost= 1e-13,scale = FALSE)
#tuned=tune(svm, PHQ_binary ~ .,data = svm_dat, kernel="linear",ranges=list(cost=c(1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1e-00, 1e01, 1e02)),scale=FALSE)
#summary(tuned)
#print(svm_fit)
#plot(svm_fit,svm_dat[,c("Happy_Faces","Neutral_Faces","PHQ_binary")])
#plot(svm_fit,svm_dat)
summary(model_PHQ_by_prefernces)
summary(model_PHQ_by_CAT_and_prefernces)
View(model_data)
model_data$predicted=predict(model_PHQ_by_CAT_and_prefernces, model.data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
)
model.data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
model.data[c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
model.data[list("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")
model.data[c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal"),]
model.data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
model.data["Happy_Faces_initial_prefernces_ordinal"]
model.data[Happy_Faces_initial_prefernces_ordinal]
model_data$predicted=predict(model_PHQ_by_CAT_and_prefernces, model_data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")]
)
model_data$predicted=predict(model_PHQ_by_CAT_and_prefernces, model_data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")])
View(model_data)
ggplot(model_data,aes(predicted,PHQ_ordinal))+geom_point()
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(model='lm')
geom_point(shape = 16, size = 5, alpha= 0.6) +
theme_bw()
model_data$predicted=predict(model_PHQ_by_CAT_and_prefernces, model_data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")])
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(model='lm') +
geom_point(shape = 16, size = 5, alpha= 0.6) +
theme_bw()
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.6) +
theme_bw()
model_data$predicted=predict(model_PHQ_by_CAT_and_prefernces, model_data[,c("Happy_Faces_CAT_effect_ordinal","Happy_Faces_initial_prefernces_ordinal")])
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
colors=model_data$PHQ_ordinal
colors=rank(model_data$Happy_Faces_initial_prefernces_ordinal*model_data$Happy_Faces_CAT_effect_ordinal)
colfunc<-colorRampPalette(c("red","yellow"))
colormap=colfunc(length(unique(colors)))
colors=colormap[c(colors)]
ggplot(model_data,aes(predicted,PHQ_ordinal,color=colors))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=colors))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
colors=model_data$PHQ_ordinal
colors=rank(model_data$Happy_Faces_initial_prefernces_ordinal*model_data$Happy_Faces_CAT_effect_ordinal)
colfunc<-colorRampPalette(c("red","yellow"))
colormap=colfunc(length(unique(colors)))
colors=colormap[c(colors)]
colors
ggplot(model_data,aes(predicted,PHQ_ordinal,color=colors))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
colors=model_data$PHQ_ordinal
colors=rank(model_data$Happy_Faces_initial_prefernces_ordinal*model_data$Happy_Faces_CAT_effect_ordinal)
colfunc<-colorRampPalette(c("red","yellow"))
colormap2=colfunc(length(unique(colors)))
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ,colormap=colormap2))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=colormap2))+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
scale_colour_gradient(low = "#red", high = "yellow")+
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
colfunc<-colorRampPalette(c("red","yellow"))
colfunc
colormap=colfunc(length(unique(colors)))
colfunc(0)
dev.new()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
scale_colour_gradient(low = "red", high = "yellow") +
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm') +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
ggplot(model_data,aes(predicted,PHQ_ordinal,color=PHQ))+
scale_colour_gradient(low = "red", high = "yellow") +
xlab(expression("Model prediction - PHQ Rank")) +
ylab(expression("Actual PHQ Rank")) +
geom_smooth(method='lm',color="red") +
geom_point(shape = 16, size = 5, alpha= 0.8) +
theme_bw()
library("nlme")
library(lme4)
library("ggplot2")
library(rockchalk)
library("lm.beta")
library("e1071")
rm(list=ls())
# Standard error function
se = function(x) { out=sqrt((var(x, na.rm = TRUE))/(length(which(!is.na(x))))) }
## Original Sample
#For iMac
path="~/Dropbox/Experiment_Israel/Codes/Boost_faces_emotional/Output/"
# For PC
#path="  "
subjects_group1=c(301,303,307,310,314:316,323,326,328,334:336,338,340,342,345,346,351:355) # Define here your subjects' codes.
subjects_group2=c(113,304,306,309,311:312,318:322,324:325,329:331,333,337,339,341,343,344,348:350,356:359)
## Original Sample
#For iMac
path="~/Drive/Experiment_Israel/Codes/Boost_faces_emotional/Output/"
subjects_group1=c(301,303,307,310,314:316,323,326,328,334:336,338,340,342,345,346,351:355) # Define here your subjects' codes.
subjects_group2=c(113,304,306,309,311:312,318:322,324:325,329:331,333,337,339,341,343,344,348:350,356:359)
filelist=c()
for (s in subjects_group1){
filelist=c(filelist,Sys.glob(paste(path, "/*",s,"*probe_block*.txt",sep="")))
}
Data_group1=c()
for (f in filelist){
Data_group1=rbind(Data_group1,read.table(f,header=T,na.strings=c(999,999000)))
}
# Load in the data - Group II
filelist=c()
for (s in subjects_group2){
filelist=c(filelist,Sys.glob(paste(path, "/*",s,"*probe_block*.txt",sep="")))
}
Data_group2=c()
for (f in filelist){
Data_group2=rbind(Data_group2,read.table(f,header=T,na.strings=c(999,999000)))
}
# Merge both groups
Data_group1$group=1
Data_group2$group=2
Results=rbind(Data_group1,Data_group2)
Results$SubNumeric=as.numeric(gsub("[^0-9]", "",Results$subjectID)) # Convert SubjectID into numbers only
Results$Order=as.factor(2-Results$SubNumeric%%2)
Results$group=as.factor(Results$group)
Results$Outcome[Results$Outcome>1]=NA # transform no choice (999) and non Go-NoGo (99) outcome into NA
Results$PairType2[Results$PairType==1]="Happy_Faces"
Results$PairType2[Results$PairType==2]="Neutral_Faces"
Results$PairType2[Results$PairType==3]="Sanity_Happy"
Results$PairType2[Results$PairType==4]="Sanity_Neutral"
tapply(Results$Outcome,Results$PairType2,mean,na.rm=T)
tapply(Results$Outcome, list(group=Results$group,PairType2=Results$PairType2), mean,na.rm=T)
tapply(Results$Outcome, list(SubjectID=Results$subjectID,PairType2=Results$PairType2,group=Results$group), mean,na.rm=T)
tapply(Results$Outcome, list(group=Results$group,PairType2=Results$PairType2), mean,na.rm=T)
# Main analysis - logistic regression
# Group 1: Happy Faces
summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==1 )),na.action=na.omit,family=binomial))
# Effect of interst: training, generalization (merged and then by group)
effect_names=c("Group 1: Happy Faces","Group 1: Neutral Faces","Group 2: Happy Faces","Group 2: Neutral Faces")
effect_tmp_1=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_2=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_3=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_4=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_1
effect_tmp_1$coefficients[2,]
effect_tmp_1$coefficients[1,]
effects_table=as.data.frame(rbind(effect_tmp_1$coefficients[1,],effect_tmp_2$coefficients[1,],effect_tmp_3$coefficients[1,],effect_tmp_4$coefficients[1,]))
effects_table$effect=effect_names
effects_table$OR=exp(effects_table$Estimate)
effects_table$OR_CI_lower = exp(effects_table$Estimate - 1.96*effects_table$`Std. Error`)
effects_table$OR_CI_upper = exp(effects_table$Estimate + 1.96*effects_table$`Std. Error`)
effects_table
effect_tmp_2
effects_table$effect=effect_names
effects_table$OR=exp(effects_table$Estimate)
effects_table$OR_CI_lower = exp(effects_table$Estimate - 1.96*effects_table$`Std. Error`)
effects_table$OR_CI_upper = exp(effects_table$Estimate + 1.96*effects_table$`Std. Error`)
effects_table
effect_tmp_2
effects_table
effect_tmp_4
effect_tmp_3
effect_names=c("Group 1: Happy Faces","Group 1: Neutral Faces","Group 2: Happy Faces","Group 2: Neutral Faces","differential effect - group Happy faces","differential effect - group Neutral faces")
effect_tmp_1=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_2=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_3=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_4=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_5==summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==1)),na.action=na.omit,family=binomial))
effect_tmp_6=summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==2)),na.action=na.omit,family=binomial))
effects_table=as.data.frame(rbind(effect_tmp_1$coefficients[1,],effect_tmp_2$coefficients[1,],effect_tmp_3$coefficients[1,],effect_tmp_4$coefficients[1,],effect_tmp_5$coefficients[2,],effect_tmp_6$coefficients[2,]))
effects_table$effect=effect_names
effects_table$OR=exp(effects_table$Estimate)
effects_table$OR_CI_lower = exp(effects_table$Estimate - 1.96*effects_table$`Std. Error`)
effects_table$OR_CI_upper = exp(effects_table$Estimate + 1.96*effects_table$`Std. Error`)
# Effect of interst: training, generalization (merged and then by group)
effect_names=c("Group 1: Happy Faces","Group 1: Neutral Faces","Group 2: Happy Faces","Group 2: Neutral Faces","differential effect - group Happy faces","differential effect - group Neutral faces")
effect_tmp_1=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_2=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_3=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_4=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_5==summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==1)),na.action=na.omit,family=binomial))
effect_tmp_6=summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==2)),na.action=na.omit,family=binomial))
effect_tmp_5=summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==1)),na.action=na.omit,family=binomial))
effect_tmp_6=summary(glmer(Outcome ~ 1 + group + (1|subjectID),data=subset(Results,(Results$PairType==2)),na.action=na.omit,family=binomial))
effects_table=as.data.frame(rbind(effect_tmp_1$coefficients[1,],effect_tmp_2$coefficients[1,],effect_tmp_3$coefficients[1,],effect_tmp_4$coefficients[1,],effect_tmp_5$coefficients[2,],effect_tmp_6$coefficients[2,]))
effects_table$effect=effect_names
effects_table$OR=exp(effects_table$Estimate)
effects_table$OR_CI_lower = exp(effects_table$Estimate - 1.96*effects_table$`Std. Error`)
effects_table$OR_CI_upper = exp(effects_table$Estimate + 1.96*effects_table$`Std. Error`)
effects_table
effect_tmp_5
effect_tmp_6
# Full model - Interaction: group and stimulus
summary(glmer(Outcome ~ 1 + PairType * group + (1|subjectID),data=subset(Results,(Results$PairType<=2)),na.action=na.omit,family=binomial))
# Interaction: group and stimulus in sanity
summary(glmer(Outcome ~ 1 + PairType + group + (1|subjectID),data=subset(Results,(Results$PairType==3|Results$PairType==4)),na.action=na.omit,family=binomial))
# Full model - Interaction: group and stimulus
summary(glmer(Outcome ~ 1 + PairType * group + (1|subjectID),data=subset(Results,(Results$PairType<=2)),na.action=na.omit,family=binomial))
# Effect of interst: training, generalization (merged and then by group)
effect_names=c("Group 1: Happy Faces","Group 1: Neutral Faces","Group 2: Happy Faces","Group 2: Neutral Faces","Group-Stimuli interaction")
effect_tmp_1=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_2=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==1 )),na.action=na.omit,family=binomial))
effect_tmp_3=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==1 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_4=summary(glmer(Outcome ~ 1 + (1|subjectID),data=subset(Results,(Results$PairType==2 & group==2 )),na.action=na.omit,family=binomial))
effect_tmp_5=summary(glmer(Outcome ~ 1 + PairType * group + (1|subjectID),data=subset(Results,(Results$PairType<=2)),na.action=na.omit,family=binomial))
effects_table=as.data.frame(rbind(effect_tmp_1$coefficients[1,],effect_tmp_2$coefficients[1,],effect_tmp_3$coefficients[1,],effect_tmp_4$coefficients[1,],effect_tmp_5$coefficients[4,]))
effects_table$effect=effect_names
effects_table$OR=exp(effects_table$Estimate)
effects_table$OR_CI_lower = exp(effects_table$Estimate - 1.96*effects_table$`Std. Error`)
effects_table$OR_CI_upper = exp(effects_table$Estimate + 1.96*effects_table$`Std. Error`)
effects_table
# Clear workspace
rm(list=ls())
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Required R package
library("pwr")
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=0.80
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generaliztion.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",power.power,"power: n =",minimal_n))
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Clear workspace
rm(list=ls())
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Required R package
library("pwr")
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=0.80
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generaliztion.delta)
mean(Power_Data$Generaliztion.delta)
View(Power_Data)
mean(Power_Data$Generalization.delta)
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generalization.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",power.power,"power: n =",minimal_n))
# Power Analysis - Generalization of learning across affective stimuli
# ==================================================================================
# Salomon, T., Nahum, R., Tamir, M., Schonberg, T.
# Instructions:
# ------------
# 1. Download the data file "Power_Data.Rda"
# 2. Copy the local path of the data file to 'my_path' variable (line 25); e.g. "~/Documents/My_directory/"
# 3. If you do not have the 'pwr' package, install it (type to Consule 'install.packages("pwr")').
# Code description:
# ---------------------
# Power Analysis based on previous CAT experiments published in Schonberg et al. (2014), and an additional pilot study (unpublished).
# The file 'probe_data_power_analysis.Rda' includes probe reslts from five experiments: Four experiments published in Schonberg et al.(2014), and
# an additional pilot experiment (unpublished), conducted in our lab prior to data acquisition.
# In the dataframe, each line one participant, 'PropChoseHVGO' field indicate the proportion of probe choices in which the participant chose
# high-value Go item over another high-value No-Go item.
# For the power analysis with alpha=0.05, power=0.8, in a one sample t-test the proportion of trials in which participants chose the Go items is compared
# to 0.5 (50% chance level), as the expected mean proportion under the null hypothesis.
# Clear workspace
rm(list=ls())
# Define the local path where 'probe_data.Rda' is saved.
my_path="~/Documents/My_directory/"
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Required R package
library("pwr")
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=0.80
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generalization.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",power.power,"power: n =",minimal_n,"per each group"))
sqrt(0.8)
# Power Analysis - Generalization of learning across affective stimuli
# ==================================================================================
# Salomon, T., Nahum, R., Tamir, M., Schonberg, T.
# Instructions:
# ------------
# 1. Download the data file "Power_Data.Rda"
# 2. Copy the local path of the data file to 'my_path' variable (line 25); e.g. "~/Documents/My_directory/"
# 3. If you do not have the 'pwr' package, install it (type to Consule 'install.packages("pwr")').
# Code description:
# ---------------------
# Power Analysis based on previous CAT experiments published in Schonberg et al. (2014), and an additional pilot study (unpublished).
# The file 'probe_data_power_analysis.Rda' includes probe reslts from five experiments: Four experiments published in Schonberg et al.(2014), and
# an additional pilot experiment (unpublished), conducted in our lab prior to data acquisition.
# In the dataframe, each line one participant, 'PropChoseHVGO' field indicate the proportion of probe choices in which the participant chose
# high-value Go item over another high-value No-Go item.
# For the power analysis with alpha=0.05, power=0.8, in a one sample t-test the proportion of trials in which participants chose the Go items is compared
# to 0.5 (50% chance level), as the expected mean proportion under the null hypothesis.
# Clear workspace
rm(list=ls())
# Define the local path where 'probe_data.Rda' is saved.
my_path="~/Documents/My_directory/"
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Required R package
library("pwr")
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=sqrt(0.80)
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generalization.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",power.power,"power: n =",minimal_n,"per each group"))
# Power Analysis - Generalization of learning across affective stimuli
# ==================================================================================
# Salomon, T., Nahum, R., Tamir, M., Schonberg, T.
# Instructions:
# ------------
# 1. Download the data file "Power_Data.Rda"
# 2. Copy the local path of the data file to 'my_path' variable (line 25); e.g. "~/Documents/My_directory/"
# 3. If you do not have the 'pwr' package, install it (type to Consule 'install.packages("pwr")').
# Code description:
# ---------------------
# Power Analysis based on previous CAT experiments published in Schonberg et al. (2014), and an additional pilot study (unpublished).
# The file 'probe_data_power_analysis.Rda' includes probe reslts from five experiments: Four experiments published in Schonberg et al.(2014), and
# an additional pilot experiment (unpublished), conducted in our lab prior to data acquisition.
# In the dataframe, each line one participant, 'PropChoseHVGO' field indicate the proportion of probe choices in which the participant chose
# high-value Go item over another high-value No-Go item.
# For the power analysis with alpha=0.05, power=0.8, in a one sample t-test the proportion of trials in which participants chose the Go items is compared
# to 0.5 (50% chance level), as the expected mean proportion under the null hypothesis.
# Clear workspace
rm(list=ls())
# Define the local path where 'probe_data.Rda' is saved.
my_path="~/Documents/My_directory/"
my_path="~/Drive/Experiment_Israel/Codes/Boost_IAPS_generalization_II/Analysis/PowerAnalysis/"
# Define the file you want to analyze
my_datafile="Power_Data.Rda"
# Load Data
load(paste(my_path,my_datafile,sep = ""))
# Required R package
library("pwr")
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=sqrt(0.80)
# Compute Cohen's d values for each sample
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generalization.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",round(power.power,3),"power: n =",minimal_n,"per each group"))
View(Power_Data)
View(Power_Data)
# Power analysis variables
power.null_mean=0
power.type="paired"
power.alpha=0.05
power.power=sqrt(0.80) # 80% power to detect an effect in both groups (0.8^0.5 power to detect an effect in each group)
# Compute Cohen's d values for each group
Cohens_d = function(x) {(mean(x)-power.null_mean)/sd(x)}
power.d=Cohens_d(Power_Data$Generalization.delta)
# Compute a minimal n - Dependent samples t-test
pwr_analysis_results=pwr.t.test(d = power.d,type = power.type, sig.level = power.alpha, power = power.power, alternative = "two.sided")
minimal_n=ceiling(pwr_analysis_results$n)
print(paste("Minimal n required for",round(power.power,3),"power: n =",minimal_n,"per each group"))
